{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727e402e-3031-4b41-880c-7b14e804db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loader - BraTS\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../brats-mri\")\n",
    "sys.path.append('../brats-mri/brats_mri_class_cond/')\n",
    "import monai\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from monai.utils import first\n",
    "from generative.inferers import LatentDiffusionInferer\n",
    "from generative.networks.schedulers import DDIMScheduler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pretrained import load_autoencoder, load_unet\n",
    "import utils\n",
    "\n",
    "BUNDLE = '../brats-mri/brats_mri_class_cond/'\n",
    "sys.path.append(BUNDLE)\n",
    "from scripts.inferer import LatentDiffusionInfererWithClassConditioning\n",
    "\n",
    "def get_monai_autoencoder(bundle_target, training_args, weights_override_path):\n",
    "    # load autoencoder\n",
    "    autoencoder = load_autoencoder(bundle_target,\n",
    "                                   override_model_cfg_json=training_args.config,\n",
    "                                   override_weights_load_path=weights_override_path)    \n",
    "    return autoencoder\n",
    "\n",
    "def get_monai_unet(bundle_target, training_args, weights_override_path):\n",
    "    unet = load_unet(bundle_target,\n",
    "                     context_conditioning=training_args.conditioning == 'context',\n",
    "                     override_model_cfg_json=training_args.config,\n",
    "                     override_weights_load_path=weights_override_path,\n",
    "                     use_conditioning=True)\n",
    "    return unet\n",
    "    \n",
    "def get_monai_model_dict(bundle_target, training_args, autoencoder_weights_path, unet_weights_path):\n",
    "    monai_dict = {}\n",
    "    training_args = torch.load(os.path.join(output_dir, training_name, 'training_args'))\n",
    "    monai_dict['autoencoder'] = get_monai_autoencoder(bundle_target, training_args, autoencoder_weights_path)\n",
    "    monai_dict['unet'] = get_monai_unet(bundle_target, training_args, unet_weights_path)\n",
    "    \n",
    "    # set scheduler\n",
    "    config = utils.model_config(bundle_target, training_args.config)\n",
    "    monai_dict['scheduler'] = config.get_parsed_content('noise_scheduler')\n",
    "    # set inferer\n",
    "    if training_args.conditioning in ['context', 'none']:\n",
    "        monai_dict['inferer'] = LatentDiffusionInferer(scheduler=scheduler, scale_factor=scale_factor)\n",
    "    else:\n",
    "        monai_dict['inferer'] = LatentDiffusionInfererWithClassConditioning(scheduler=scheduler, scale_factor=scale_factor)\n",
    "    return monai_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa23527-acae-49b6-865a-cef20b7352cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix plotter\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def labels_to_human_labels(labels, human_labels_list):\n",
    "    human_labels = [human_labels_list[int(x.detach().cpu().numpy())] for x in labels]  \n",
    "    return human_labels\n",
    "\n",
    "def plot_confusion_matrix(true_labels, labels_pred, human_labels_list):\n",
    "    true_labels_readable = labels_to_human_labels(true_labels, human_labels_list)\n",
    "    labels_pred_readable = labels_to_human_labels(labels_pred, human_labels_list)\n",
    "    cm = confusion_matrix(true_labels_readable, labels_pred_readable, labels=human_labels_list)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=human_labels_list)\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    disp.plot(ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e31831-065b-4d42-80b7-f656dd9bff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"../diffusion_classifier\")\n",
    "from ldm_classifier_brats import MonaiLdmClassifier\n",
    "\n",
    "def strip_epoch_num_from_ckpt(ckpt_full_path):\n",
    "  ckpt_name = ckpt_full_path.split('/')[-1]  \n",
    "  epoch_num = int(ckpt_name.split(\".\")[0].split(\"_\")[-1])\n",
    "  return epoch_num\n",
    "\n",
    "def get_monai_training_ckpt_files(output_dir, training_name):\n",
    "  ckpt_dir = os.path.join(output_dir, training_name)\n",
    "  autoencoder_ckpt_files = [os.path.join(ckpt_dir, ckpt) for ckpt in os.listdir(ckpt_dir) if 'autoencoder' in ckpt]\n",
    "  unet_ckpt_files = [os.path.join(ckpt_dir, ckpt) for ckpt in os.listdir(ckpt_dir) if 'diffusion' in ckpt]\n",
    "  return autoencoder_ckpt_files, unet_ckpt_files\n",
    "\n",
    "def evaluate_accuracy_over_epochs(output_dir, \n",
    "                                  training_name, \n",
    "                                  dataset, \n",
    "                                  t_sampling_stride = 50,\n",
    "                                  n_trials = 1\n",
    "                                 ):\n",
    "    # create classification results dir under the training dir\n",
    "    clf_dir = os.path.join(output_dir, training_name, 'classification')\n",
    "    if not os.path.exists(clf_dir):\n",
    "        os.makedirs(clf_dir)\n",
    "    n_pred_files = len(os.listdir(clf_dir))\n",
    "\n",
    "    clf_res_per_epoch = {'dataset': dataset}\n",
    "    # prepare files\n",
    "    autencoder_ckpt_files, unet_ckpt_files = get_monai_training_ckpt_files(output_dir, training_name)\n",
    "    # loop over ckpts\n",
    "    for autoenc_ckpt, unet_ckpt in zip(autencoder_ckpt_files, unet_ckpt_files):\n",
    "        epoch_num = strip_epoch_num_from_ckpt(autoenc_ckpt)\n",
    "        # load model\n",
    "        model_dict = get_monai_model_dict(BUNDLE, training_args, autoenc_ckpt, unet_ckpt)\n",
    "        # instantiate ldm classifier\n",
    "        ldm_clf = MonaiLdmClassifier(**model_dict)\n",
    "        # run classification\n",
    "        l2_labels_pred, l1_labels_pred, true_labels = ldm_clf.classify_dataset(dataset=ds,\n",
    "                                                                               batch_size=1,\n",
    "                                                                               n_trials=n_trials,\n",
    "                                                                               t_sampling_stride=t_sampling_stride)\n",
    "        # save results\n",
    "        clf_res_per_epoch[epoch_num] = {\n",
    "          'true_labels': true_labels,\n",
    "          'l1_pred_labels': l1_labels_pred,\n",
    "          'l2_pred_labels': l2_labels_pred,\n",
    "        }\n",
    "        with open(os.path.join(clf_dir, f'predictions_{n_pred_files}'), 'wb') as f:\n",
    "            pickle.dump(clf_res_per_epoch, f)\n",
    "        \n",
    "        # delete model\n",
    "        del model\n",
    "        del ldm_clf\n",
    "    \n",
    "    return clf_res_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a580e0d3-e221-4bf6-83db-11b7916d24bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# TODO - define the correct path to data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../brats-mri/brats_mri_class_cond/scripts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mct_rsna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CTSubset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from torchvision import transforms\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/deep-learning-course/Project/figures/../brats-mri/brats_mri_class_cond/scripts/ct_rsna.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCTDataset\u001b[39;00m(Dataset):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_dir, labels_file, size, flip_prob) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/monai/lib/python3.11/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/monai/lib/python3.11/site-packages/torchvision/_meta_registrations.py:25\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/monai/lib/python3.11/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# TODO - define the correct path to data\n",
    "sys.path.append('../brats-mri/brats_mri_class_cond/scripts')\n",
    "from ct_rsna import CTSubset\n",
    "# from torchvision import transforms\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cdbdcf-a8af-4971-85cd-6f950715e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "train_dir = './data/ct-rsna/train'\n",
    "val_dir = './data/ct-rsna/validation'\n",
    "\n",
    "subset_len = 1\n",
    "ds = CTSubset(data_dir=val_dir, labels_file='validation_set_dropped_nans.csv', size=256, flip_prob=0., subset_len=subset_len)\n",
    "\n",
    "# LDM classifier params\n",
    "t_sampling_stride = 50\n",
    "n_trials = 1\n",
    "\n",
    "# training dir\n",
    "output_dir = './data/outputs'\n",
    "training_name = 'brats001'\n",
    "\n",
    "# evaluate\n",
    "clf_res_per_epoch = evaluate_accuracy_over_epochs(output_dir, \n",
    "                                                  training_name, \n",
    "                                                  ds, \n",
    "                                                  t_sampling_stride,\n",
    "                                                  n_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
