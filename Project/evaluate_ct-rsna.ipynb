{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUmmV5ZvrPbP"
   },
   "source": [
    "# Class-Conditional Synthesis with Latent Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./latent-diffusion')\n",
    "sys.path.append('./taming-transformers')\n",
    "\n",
    "TRAIN_RUN_NAME = '2024-04-24T14-05-21_ct-rsna'\n",
    "CKPT_NAME = 'checkpoints/last.ckpt'\n",
    "\n",
    "train_outputs_dir = os.path.join('./data/outputs/', TRAIN_RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect train run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_file_path = os.path.join(train_outputs_dir, \"testtube/version_0/metrics.csv\")\n",
    "metrics_df = pd.read_csv(metrics_file_path)\n",
    "# metrics_df = plot_training_metrics(model_train_dir)\n",
    "\n",
    "epoch = metrics_df['epoch'].dropna().values\n",
    "\n",
    "train_loss = metrics_df['train/loss_simple_epoch'].dropna().values\n",
    "val_loss = metrics_df['val/loss_simple'].dropna().values\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss vs. epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(train_outputs_dir, \"images/val\")\n",
    "\n",
    "if os.path.exists(images_dir) and os.listdir(images_dir):\n",
    "    N = 10\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i, img_path in enumerate(os.listdir(images_dir)):\n",
    "        img = plt.imread(os.path.join(images_dir, img_path))\n",
    "    \n",
    "        plt.subplot(1, N, i+1)\n",
    "        plt.imshow(img, cmap='gray', vmin=-1., vmax=1.)\n",
    "    \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        i += 1\n",
    "        if i >= N:\n",
    "            break\n",
    "        \n",
    "                   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = os.path.join('./data/outputs/', os.path.join(TRAIN_RUN_NAME, CKPT_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fnGwQRhtyBhb"
   },
   "outputs": [],
   "source": [
    "#@title loading utils\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    config = OmegaConf.load(\"latent-diffusion/configs/latent-diffusion/ct-rsna.yaml\")  \n",
    "    model = load_model_from_config(config, checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPnyd-XUKbfE",
    "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954"
   },
   "outputs": [],
   "source": [
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "\n",
    "model = get_model()\n",
    "sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIEAhY8AhUrh"
   },
   "source": [
    "And go. Quality, sampling speed and diversity are best controlled via the `scale`, `ddim_steps` and `ddim_eta` variables. As a rule of thumb, higher values of `scale` produce better samples at the cost of a reduced output diversity. Furthermore, increasing `ddim_steps` generally also gives higher quality samples, but returns are diminishing for values > 250. Fast sampling (i e. low values of `ddim_steps`) while retaining good quality can be achieved by using `ddim_eta = 0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jcbqWX2Ytu9t",
    "outputId": "3b7adde0-d80e-4c01-82d2-bf988aee7455"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "classes = list(range(6))   # define classes to be sampled here\n",
    "n_samples_per_class = 6\n",
    "\n",
    "ddim_steps = 20\n",
    "ddim_eta = 0.0\n",
    "scale = 3.0   # for unconditional guidance\n",
    "\n",
    "\n",
    "all_samples = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model.ema_scope():\n",
    "        uc = model.get_learned_conditioning(\n",
    "            {model.cond_stage_key: torch.tensor(n_samples_per_class*[5]).to(model.device)}\n",
    "            )\n",
    "        \n",
    "        for class_label in classes:\n",
    "            print(f\"rendering {n_samples_per_class} examples of class '{class_label}' in {ddim_steps} steps and using s={scale:.2f}.\")\n",
    "            xc = torch.tensor(n_samples_per_class*[class_label])\n",
    "            c = model.get_learned_conditioning({model.cond_stage_key: xc.to(model.device)})\n",
    "            \n",
    "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                             conditioning=c,\n",
    "                                             batch_size=n_samples_per_class,\n",
    "                                             shape=[3, 64, 64],\n",
    "                                             verbose=False,\n",
    "                                             unconditional_guidance_scale=scale,\n",
    "                                             unconditional_conditioning=uc, \n",
    "                                             eta=ddim_eta)\n",
    "\n",
    "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, \n",
    "                                         min=0.0, max=1.0)\n",
    "            all_samples.append(x_samples_ddim)\n",
    "\n",
    "\n",
    "# display as grid\n",
    "grid = torch.stack(all_samples, 0)\n",
    "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "grid = make_grid(grid, nrow=n_samples_per_class)\n",
    "\n",
    "# to image\n",
    "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "Image.fromarray(grid.astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
